# helm install prom prometheus-community/kube-prometheus-stack -f ./values.prometheus.yaml -n prometheus --create-namespace
# helm upgrade prom prometheus-community/kube-prometheus-stack -f ./values.prometheus.yaml -n prometheus
# helm uninstall prom -n prometheus
grafana:
  ingress:
    enabled: true
    # className: "traefik"
    annotations:
      kubernetes.io/ingress.class: traefik
      kubernetes.io/tls-acme: "true"
    hosts:
      - k8s.louis030195.com
    path: / #/grafana
    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
      - secretName: awesome-key
        hosts:
        - k8s.louis030195.com
  # TODO
  # grafana.ini:
  # smtp:
  #   enabled: true
  #   host: smtp.gmail.com:587 
  #   user: username@gmail.com
  #   # If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
  #   password: """pass123#"""
  #   skip_verify: true
  #   from_address: admin@grafana.kifarunix-demo.com
  #   from_name: Grafana
  
# nodeExporter:
#   ## Additional containers for export metrics to text file
#   ##
#   sidecars: 
#     - name: nvidia-dcgm-exporter
#       image: nvidia/dcgm-exporter:2.2.9


## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
## as specified in the official Prometheus documentation:
## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
## scrape configs are going to break Prometheus after the upgrade.
##
## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
##
additionalScrapeConfigs:
- job_name: gpu-metrics
  scrape_interval: 1s
  metrics_path: /metrics
  scheme: http
  kubernetes_sd_configs:
  - role: node
    # namespaces:
    #   names:
    #   - gpu-operator-resources
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_node_name]
    action: replace
    target_label: kubernetes_node
## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
## prometheus resource to be created with selectors based on values in the helm deployment,
## which will also match the servicemonitors created
##
serviceMonitorSelectorNilUsesHelmValues: false
